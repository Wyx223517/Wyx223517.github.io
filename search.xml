<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>张量分解</title>
    <url>/posts/e05ab48b.html</url>
    <content><![CDATA[<p>张量又称多维数组，是矩阵的高维推广。与传统的矩阵（二维数据）不同，张量能够处理更高维度的数据，因此它是多维数据表示的理想工具。</p>
<h2 id="张量的概念"><a href="#张量的概念" class="headerlink" title="张量的概念"></a>张量的概念</h2><div class="note info no-icon flat"><p><strong>定义 (张量):</strong><br>张量是一个多维数组，$d$ 阶张量是一个具有 $d$ 个维度的数组，表示为：</p>
<script type="math/tex; mode=display">
\mathcal{X} \in \mathbb{R}^{n_1 \times n_2 \times \cdots \times n_d}</script></div>
<p>下图展示了从标量到三阶张量的直观对比：</p>
<div style="width: 60%; margin: 0 auto;">
<img src="/posts/e05ab48b/Example_of_Tensors.png" class="" title="标量、向量、矩阵和三阶张量">
</div>

<h2 id="张量分解"><a href="#张量分解" class="headerlink" title="张量分解"></a>张量分解</h2><h3 id="Tucker-分解"><a href="#Tucker-分解" class="headerlink" title="Tucker 分解"></a>Tucker 分解</h3><p>张量分解是一种将高维张量分解为若干低维因子组合的技巧。其中一种为 <strong>Tucker 分解</strong>，它可以被视为张量的高阶主成分分析（PCA）。</p>
<p>Tucker 分解的核心思想是将张量分解为<strong>核心张量</strong>与<strong>各模式因子矩阵</strong>的乘积。</p>
<p>具体地，给定一个张量 $\mathcal{X} \in \mathbb{R}^{n_1 \times n_2 \times \cdots \times n_d}$，Tucker 分解表示为：</p>
<div class="note success no-icon flat"><p><strong>Tucker Decomposition:</strong></p>
<script type="math/tex; mode=display">
\mathcal{X} \approx \mathcal{C} \times_1 Q^1 \times_2 Q^2 \cdots \times_d Q^d = \sum_{j_1=1}^{m_1} \sum_{j_2=1}^{m_2} \cdots \sum_{j_d=1}^{m_d} c_{j_1 j_2 \dots j_d} \, \mathbf{q}^{1}_{j_1} \circ \mathbf{q}^{2}_{j_2} \circ \cdots \circ \mathbf{q}^{d}_{j_d}</script></div>
<p>其中：</p>
<ul>
<li>$\mathcal{C}\in \mathbb{R}^{m_1 \times m_2 \times \cdots \times m_d}$ 是 <strong>核心张量 (Core Tensor)</strong>，反映了不同模式之间的相互作用。</li>
<li>$Q^{k} \in \mathbb{R}^{n_k \times m_k}$ 是 <strong>因子矩阵 (Factor Matrices)</strong>。</li>
</ul>
<div style="width: 60%; margin: 0 auto;">
<img src="/posts/e05ab48b/Tucker.png" class="" title="Tucker分解示意图">
</div>

<h3 id="CP-分解"><a href="#CP-分解" class="headerlink" title="CP 分解"></a>CP 分解</h3><p>特别地，当 $m_{1} =m_{2} = \cdots =m_{d} = r$，且核心张量限制为<strong>对角型</strong>时，被称为 <strong>CP 分解</strong> (Canonical Polyadic Decomposition)。</p>
<p>它将张量近似为一组<strong>秩为 1 的张量之和</strong>。具体而言，给定一个张量 $\mathcal{X} \in \mathbb{R}^{n_1 \times n_2 \times \cdots \times n_d}$，其分解形式为：</p>
<div class="note success no-icon flat"><p><strong>CP Decomposition:</strong></p>
<script type="math/tex; mode=display">
\mathcal{X} \approx \sum_{j=1}^r \mathbf{p}_j^1 \circ \mathbf{p}_j^2 \circ \cdots \circ \mathbf{p}_j^d</script></div>
<p>其中 $\mathbf{p}_j^k$ 为 $n_k$ 维向量（$k = 1, 2, \cdots, d$）。</p>
<p><strong>优势：</strong> CP 分解将系数从 $p_{1}\times\cdots\times p_{d}$ 降低至 $r(p_{1} + p_{2} +\cdots +p_{d})$，提供了极高效的数据降维能力。</p>
<p><strong>缺点：</strong> CP 分解的最优秩的确定是一个NP-hard 的问题，实际操作时难以实现。</p>
<div style="width: 60%; margin: 0 auto;">
<img src="/posts/e05ab48b/PARAFAC.png" class="" title="PARAFAC分解示意图">
</div>

<h3 id="TT-分解"><a href="#TT-分解" class="headerlink" title="TT 分解"></a>TT 分解</h3><p><strong>TT分解的核心思想</strong></p>
<ul>
<li>假设模态的排序具有某种语义相关性。</li>
<li>将高阶张量 $\mathcal{A}$ 近似为一串 3 阶张量的链式乘积。</li>
</ul>
<div class="note success no-icon flat"><p><strong>TT Decomposition:</strong></p>
<script type="math/tex; mode=display">
\mathcal{A}(i_1, \dots, i_d) = \sum_{\alpha_0=1}^{r_0} \cdots \sum_{\alpha_d=1}^{r_d} \mathcal{G}_1(\alpha_0, i_1, \alpha_1) \mathcal{G}_2(\alpha_1, i_2, \alpha_2) \cdots \mathcal{G}_d(\alpha_{d-1}, i_d, \alpha_d)</script></div>
<p>其中 $\mathcal{G}_k \in \mathbb{R}^{r_{k-1} \times n_k \times r_k}$。$r_k$ 称为 <strong>TT-秩 (TT-rank)</strong>，且满足边界条件 $r_0 = r_d = 1$。</p>
<div style="width: 60%; margin: 0 auto;">
  <img src="/posts/e05ab48b/TT.png" class="" title="TT分解示意图">
</div>

<p><strong>优势：</strong> TT分解的好处在于随着张量阶数提升，分解后的形式仅仅是在已有分解的基础上在后面加上几节“车厢”，而相比之下Tucker分解的核心张量参数量仍是指数阶的。</p>
<p><strong>不足：</strong> 然而，TT分解高度依赖于张量维度的排列，这导致难以找到最佳的TT表示。</p>
<h3 id="TR-分解"><a href="#TR-分解" class="headerlink" title="TR 分解"></a>TR 分解</h3><p>TR 分解将高阶张量 $\mathcal{T}$ 表示为一系列三阶核心张量的<strong>循环</strong>矩阵乘积的迹。它可以看作是 TT 分解的推广（当首尾秩为 1 时退化为 TT），也可以看作是首尾相连的“环状”结构。</p>
<div class="note success no-icon flat"><p><strong>TR Decomposition:</strong></p>
<script type="math/tex; mode=display">
\mathcal{T}(i_1, i_2, \dots, i_d) = \text{Tr} \left( \mathbf{Z}_1(i_1) \mathbf{Z}_2(i_2) \cdots \mathbf{Z}_d(i_d) \right) = \text{Tr} \left( \prod_{k=1}^d \mathbf{Z}_k(i_k) \right)</script></div>
<p>其中 $\mathcal{Z}_k \in \mathbb{R}^{r_k \times n_k \times r_{k+1}}$ 为第 $k$ 个核心张量，$\mathbf{Z}_k(i_k)$ 表示核心张量 $\mathcal{Z}_k$ 的第 $i_k$ 个侧切片矩阵（大小为 $r_k \times r_{k+1}$）。</p>
<p>这里的 $\mathbf{r} = [r_1, r_2, \cdots, r_d]^{\text{T}}$ 被称为TR秩。</p>
<div style="width: 60%; margin: 0 auto;">
  <img src="/posts/e05ab48b/TR.png" class="" title="TR分解示意图">
</div>

<p>如果我们把迹运算展开，就可以得到索引形式的TR分解。</p>
<div class="note success no-icon flat"><p><strong>TR Decomposition in Index Form:</strong></p>
<script type="math/tex; mode=display">
\mathcal{T}(i_1, \dots, i_d) = \sum_{\alpha_1=1}^{r_1} \cdots \sum_{\alpha_d=1}^{r_d} \prod_{k=1}^{d} \mathcal{Z}_k(\alpha_k, i_k, \alpha_{k+1})</script></div>
<p>注意这里隐含了条件 $\alpha_{d+1} = \alpha_1$。这个公式直观地展示了每一个张量元素是如何通过核心张量中的元素相乘并求和得到的。</p>
<p>更进一步，我们可以将 TR 分解理解为一系列由 $d$ 个向量外积产生的 <strong>秩-1 张量 (Rank-1 Tensors)</strong> 之和。</p>
<div class="note success no-icon flat"><p><strong>TR Decomposition in the Tensor Form:</strong></p>
<script type="math/tex; mode=display">
\mathcal{T} = \sum_{\alpha_1=1}^{r_1} \cdots \sum_{\alpha_d=1}^{r_d} \mathbf{z}_1(\alpha_1, \alpha_2) \circ \mathbf{z}_2(\alpha_2, \alpha_3) \circ \cdots \circ \mathbf{z}_d(\alpha_d, \alpha_1)</script></div>
<p>其中：</p>
<ul>
<li>符号 $\circ$ 表示向量的外积。</li>
<li>$\mathbf{z}_k(\alpha_k, \alpha_{k+1})$ 表示核心张量 $\mathcal{Z}_k$ 的<strong>第 $(\alpha_k, \alpha_{k+1})$ 根 mode-2 纤维 (Fiber)</strong>（即固定了前后两个秩索引后，剩下的那个长度为 $n_k$ 的向量）。</li>
</ul>
<p>由此可见，通过TR分解可以将参数量从 $\mathcal{O}(n^d)$降低至 $\mathcal{O}(dnr^2)$.</p>
<p>TR 分解最显著的一个性质是其对张量维度的循环移位具有不变性。这一点将 TR 与 TT 分解严格区分开来。</p>
<div class="note primary no-icon flat"><p><strong>Theorem 1 (Circular Dimensional Permutation Invariance)</strong></p>
<p>令 $\mathcal{T} \in \mathbb{R}^{n_1 \times n_2 \times \cdots \times n_d}$ 为一个 $d$ 阶张量，其 TR 分解表示为 $\mathcal{T} = \mathfrak{R}(\mathcal{Z}_1, \mathcal{Z}_2, \dots, \mathcal{Z}_d)$。</p>
<p>如果我们定义 $\overrightarrow{\mathcal{T}}^{k}$ 为将 $\mathcal{T}$ 的维度<strong>循环右移</strong> $k$ 次后得到的新张量，$\overrightarrow{\mathcal{T}}^{k}\in\mathbb{R}^{n_{k+1} \times \cdots \times n_d \times n_1 \times \cdots \times n_k}$，那么 $\overrightarrow{\mathcal{T}}^{k}$ 的 TR 分解核心张量序列仅需做同样的循环移位：</p>
<script type="math/tex; mode=display">
\overrightarrow{\mathcal{T}}^{k} = \mathfrak{R}(\mathcal{Z}_{k+1}, \dots, \mathcal{Z}_d, \mathcal{Z}_1, \dots, \mathcal{Z}_k)</script></div>
<p><strong>证明:</strong></p>
<p>这一性质的证明直接依赖于矩阵迹运算的循环性质 $\text{Tr}(\mathbf{A}\mathbf{B}) = \text{Tr}(\mathbf{B}\mathbf{A})$。</p>
<p>根据 TR 分解的定义公式 (1)，我们可以将其重写为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{T}(i_1, i_2, \dots, i_d) &= \text{Tr}(\mathbf{Z}_1(i_1) \mathbf{Z}_2(i_2) \cdots \mathbf{Z}_d(i_d)) \\
&= \text{Tr}(\mathbf{Z}_2(i_2) \mathbf{Z}_3(i_3) \cdots \mathbf{Z}_d(i_d) \mathbf{Z}_1(i_1)) \\
&\quad \vdots \\
&= \text{Tr}(\mathbf{Z}_{k+1}(i_{k+1}) \cdots \mathbf{Z}_d(i_d) \mathbf{Z}_1(i_1) \cdots \mathbf{Z}_k(i_k))
\end{aligned}</script><p>这表明，如果我们改变张量维度的物理顺序（例如将前 $k$ 个维度移到末尾），我们只需要相应地循环移动核心张量的顺序，而不需要重新计算核心张量的值。</p>
]]></content>
      <categories>
        <category>统计机器学习</category>
      </categories>
      <tags>
        <tag>毕业论文</tag>
        <tag>CP 分解</tag>
        <tag>Tucker 分解</tag>
        <tag>TT 分解</tag>
        <tag>TR 分解</tag>
        <tag>张量</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformed LoRA via Tensor Decomposition</title>
    <url>/posts/41219a96.html</url>
    <content><![CDATA[<p>此篇博客来源于毕设课题的参考文献，复现论文<a href="https://arxiv.org/abs/2501.08727">Transformed Low-rank Adaptation via Tensor Decomposition and Its Applications to Text-to-image Models</a>，将简要记录论文第三节提出的模型。</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h3><p>我们使用方括号来表示数组的元素或切片。例如，给定一个张量 $\mathbf{X} \in \mathbb{R}^{I \times J \times K}$，我们定义其矩阵切片为 $\boldsymbol{X}_{i::} = \mathbf{X}[i, :, :] \in \mathbb{R}^{J \times K}$，向量切片为 $\boldsymbol{x}_{ij:} = \mathbf{X}[i, j, :] \in \mathbb{R}^{K}$，标量元素为 $x_{ijk} = \mathbf{X}[i, j, k] \in \mathbb{R}$。我们用 $\boldsymbol{I}_I$ 表示形状为 $I \times I$ 的单位矩阵。Kronecker 积（克罗内克积）用 $\otimes$ 表示。矩阵的迹用 $\text{tr}(\cdot)$ 表示。$\text{diag}(\boldsymbol{m})$ 表示对角元素为 $\boldsymbol{m}$ 的对角矩阵。</p>
<div class="note info no-icon flat"><p><strong>Little Endian 约定:</strong><br>对于张量化的索引，采用小端序（little-endian）约定。对于向量 $\boldsymbol{x} \in \mathbb{R}^I$（其中 $I = \prod_{d=1}^D I_d$），若将其张量化为 $\mathbf{X} \in \mathbb{R}^{I_1 \times \cdots \times I_D}$，则有 $\boldsymbol{x}[\overline{i_1 \dots i_D}] = \mathbf{X}[i_1, \dots, i_D]$，其中：</p>
<script type="math/tex; mode=display">
\overline{i_1 \dots i_D} = i_1 + (i_2 - 1)I_1 + (i_3 - 1)I_1I_2 + \cdots + (i_D - 1)I_1 \cdots I_{D-1}</script></div>
<h3 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h3><p>考虑一个线性层 $\boldsymbol{y} = \boldsymbol{W}_0 \boldsymbol{x}$，其中权重的形状为 $I \times I$。</p>
<div class="note success no-icon flat"><p><strong>Low-rank Adaptation：</strong><br>给定预训练权重 $\boldsymbol{W}_0$，LoRA 旨在学习一个具有相同形状的加性适应项 $\boldsymbol{\Delta}$。</p>
<script type="math/tex; mode=display">
\boldsymbol{y}' = (\boldsymbol{W}_0 + \boldsymbol{\Delta})\boldsymbol{x}, \quad \text{s.t. } \boldsymbol{\Delta} = \boldsymbol{B}\boldsymbol{A},</script></div>
<p>其中，该适应项通过低秩矩阵分解进行参数化，以减少可训练参数。低秩矩阵 $\boldsymbol{B} \in \mathbb{R}^{I \times R}$ 和 $\boldsymbol{A} \in \mathbb{R}^{R \times I}$ 通过给定的微调任务进行优化，它们总共有 $2IR$ 个可训练参数；如果 $R \ll I$，这一数量远小于原始大小 $I^2$。然而，理想的微调权重可能并不是低秩的。</p>
<h3 id="OFT"><a href="#OFT" class="headerlink" title="OFT"></a>OFT</h3><div class="note success no-icon flat"><p><strong>Orthogonal fine-tunning：</strong><br>引入了预训练权重的正交变换来进行适应：</p>
<script type="math/tex; mode=display">
\boldsymbol{y}' = (\boldsymbol{W}_0 \boldsymbol{T})\boldsymbol{x},</script><p>其中 $\boldsymbol{T}$ 是一个形状为 $I \times I$ 的可训练正交矩阵。</p>
</div>
<p>然而，由于 $\boldsymbol{T}$ 尺寸巨大，直接优化它在计算上是不可行的。在一些研究中中，$\boldsymbol{T}$ 被参数化为块对角矩阵，这对于较小的参数预算来说极其稀疏。这种稀疏性被认为减少了神经元之间的信息传递和连接，这对微调是有害的。</p>
<h3 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h3><p>LoRA 背后的假设是差异 $\boldsymbol{\Delta}_{\ast} = \boldsymbol{W}_{\ast} - \boldsymbol{W}_0$ 是低秩的。然而，在实际的大型基础模型中，这一假设往往难以成立。</p>
<p>为了解决这个问题，我们首先对 $\boldsymbol{W}_0$ 应用一个可学习的线性变换 $\boldsymbol{T}$，使 $\boldsymbol{W}_0$ 与 $\boldsymbol{W}_*$ 对齐，然后用另一个紧凑结构来近似残差部分。此时，差异变为 $\boldsymbol{\Delta}’_{\ast} = \boldsymbol{W}_{\ast} - \boldsymbol{W}_0\boldsymbol{T}$。变换后，$\boldsymbol{\Delta}’_{\ast}$ 的秩应小于原始 $\boldsymbol{\Delta}_{\ast}$ 的秩，从而使我们可以使用更紧凑的结构来近似该残差部分。</p>
<p>整体的微调结构变为：</p>
<div class="note info no-icon flat"><script type="math/tex; mode=display">
\boldsymbol{y}' = (\boldsymbol{W}_0 \boldsymbol{T} + \boldsymbol{\Delta})\boldsymbol{x},</script></div>
<p>其中 $\boldsymbol{T}$ 和 $\boldsymbol{\Delta}$ 是可学习的紧凑参数化形式。为了满足不同的需求和期望属性，我们分别为变换 $\boldsymbol{T}$ 设计了 TRM 形式，并为残差 $\boldsymbol{\Delta}$ 设计了 TR 形式。</p>
<h2 id="张量分解"><a href="#张量分解" class="headerlink" title="张量分解"></a>张量分解</h2><h3 id="对变化矩阵的TRM分解"><a href="#对变化矩阵的TRM分解" class="headerlink" title="对变化矩阵的TRM分解"></a>对变化矩阵的TRM分解</h3><h4 id="TRM的原理"><a href="#TRM的原理" class="headerlink" title="TRM的原理"></a>TRM的原理</h4><p>由于目标权重空间（例如微调任务的最优权重）不太可能是低秩的，我们假设 $\boldsymbol{T}$ 也具有<strong>满秩结构 (full-rank structure)</strong>。此外，为了解决 OFT 中的稀疏性问题，$\boldsymbol{T}$ 期望具有<strong>密集元素 (dense entries)</strong>。由于该变换矩阵尺寸巨大，它必须由参数高效的结构来表示。</p>
<p>为了满足上述要求，我们采用<strong>张量环矩阵 (Tensor-Ring Matrix, TRM)</strong> 形式的张量分解方法。</p>
<p>给定一个矩阵 $\boldsymbol{T} \in \mathbb{R}^{I \times J}$，假设 $I = \prod_{d=1}^D I_d$，$J = \prod_{d=1}^D J_d$，并且它可以被重排（张量化）为许多子数组。TRM 将该矩阵分解为 $D$ 个 4 阶因子 $\mathbf{A}^d \in \mathbb{R}^{I_d \times J_d \times R_d \times R_{d+1}}, \forall d = 1, \dots, D$ 的收缩（contractions），这些因子被称为核心张量 (core tensors)。</p>
<p>序列 $[R_1, \dots, R_{D+1}]$（其中 $R_{D+1} = R_1$）被称为 TR 秩。为简单起见，我们在本文中假设 $R = R_1 = \cdots = R_{D+1}$ 且 $I = J$。</p>
<div class="note success no-icon flat"><p><strong>定义（张量的矩阵化）</strong> 设 $\boldsymbol{T} \in \mathbb{R}^{n_1 \times n_2 \times \cdots \times n_d}$ 为一个 $d$ 阶张量。$\boldsymbol{T}$ 的 $k$-展开 ($k$-unfolding) 是一个矩阵，记作 $\mathbf{T}_{\langle k \rangle}$，其尺寸为 $\prod_{i=1}^k n_i \times \prod_{i=k+1}^d n_i$，其元素为：</p>
<script type="math/tex; mode=display">
\boldsymbol{T}_{\langle k \rangle}(\overline{i_1 \cdots i_k}, \overline{i_{k+1} \cdots i_d}) = \boldsymbol{T}(i_1, i_2, \dots, i_d), \tag{6}</script></div>
<p>其中，前 $k$ 个索引枚举 $\mathbf{T}_{\langle k \rangle}$ 的行，而后 $d-k$ 个索引枚举其列。</p>
<p>TRM 假设矩阵的每个元素通过以下方式计算：</p>
<div class="note info no-icon flat"><p><strong>TRM分解：</strong><br>采用了矩阵张量化的技巧，先将矩阵张量化后再作分解。</p>
<script type="math/tex; mode=display">
\boldsymbol{T}[\overline{i_1 \cdots i_D}, \overline{j_1 \cdots j_D}] = \text{tr}(\mathbf{A}^1[i_1, j_1, :, :] \cdots \mathbf{A}^D[i_D, j_D, :, :]).</script></div>
<p>为简便起见，我们将 TRM 格式记为 $\boldsymbol{T} = \text{TRM}(\mathbf{A}^{1:D})$，其中 $\mathbf{A}^{1:D}$ 表示 $\{\mathbf{A}^1, \dots, \mathbf{A}^D\}$。</p>
<p>TRM 对于表示我们需要的矩阵是一个高度紧性的分解形式。若 $\mathbf{A}^{1:D}$是稠密且满秩的，那么$\boldsymbol{T} = \text{TRM}(\mathbf{A}^{1:D})$也是一样的。</p>
<blockquote>
<p>在参数上，可以计算得到 TRM 的储存开销是$\mathcal{O}(DI^{2/D}R^2)$。 通过调整合适的超参数$D$和$R$，开销将远小于原来的 $\mathcal{O}(I^2)$。</p>
</blockquote>
<h4 id="如何初始化"><a href="#如何初始化" class="headerlink" title="如何初始化"></a>如何初始化</h4><p>为保证这个模型和原模型的初始化一致，我们需要保证$T$被初始化为单位矩阵，而我们可以按照如下方法构造单位矩阵。</p>
<div class="note info no-icon flat"><p><strong>Proposition 1</strong><br>如果我们按照如下方式初始化TRM的每一个因子：</p>
<script type="math/tex; mode=display">
\mathbf{A}^d[:, :, r, r'] = \boldsymbol{I}_{I_d} / R, \quad \forall d = 1, \dots, D, \text{ 且 } r, r' = 1, \dots, R,</script><p>则生成的 TRM $\boldsymbol{T}$ 是一个单位矩阵。</p>
</div>
<p><strong>证明：</strong></p>
<p>根据 TRM 的定义，我们有：</p>
<script type="math/tex; mode=display">
\boldsymbol{T}[\overline{i_1 \cdots i_D}, \overline{j_1 \cdots j_D}] = \text{tr}(\mathbf{A}^1[i_1, j_1, :, :] \cdots \mathbf{A}^D[i_D, j_D, :, :]).</script><p>对于 $d = 1, \dots, D$，如果 $i_d = j_d$，则 $\mathbf{A}^d[i_d, j_d, :, :]$ 的所有元素均为 $1/R$；否则为零。接下来我们可以分别讨论 $\boldsymbol{T}$ 的对角元素和非对角元素。</p>
<p><strong>1. 对于非对角元素</strong><br>即 $\overline{i_1 \cdots i_D} \neq \overline{j_1 \cdots j_D}$，意味着至少存在一个子索引 $i_d \neq j_d$。因此：</p>
<script type="math/tex; mode=display">
\boldsymbol{T}[\overline{i_1 \cdots i_D}, \overline{j_1 \cdots j_D}] = 0, \quad \text{因为当 } i_d \neq j_d \text{ 时 } \mathbf{A}^d[i_d, j_d, :, :] = \mathbf{0}.</script><p><strong>2. 对于对角元素</strong><br>即 $\overline{i_1 \cdots i_D} = \overline{j_1 \cdots j_D}$，意味着对于所有 $d = 1, \dots, D$，都有 $i_d = j_d$。此时核心张量变为：</p>
<script type="math/tex; mode=display">
\mathbf{A}^d[i_d, j_d, :, :] = \mathbf{1}_{R \times R} / R, \quad \forall d = 1, \dots, D \text{ 且 } i_d, j_d = 1, \dots, I_d,</script><p>其中 $\mathbf{1}_{R \times R}$ 是一个形状为 $R \times R$ 且所有元素均为 1 的矩阵。因此，$\boldsymbol{T}[\overline{i_1 \cdots i_D}, \overline{j_1 \cdots j_D}] = 1$，因为：</p>
<script type="math/tex; mode=display">
\text{tr}\left( \frac{\mathbf{1}_{R \times R}}{R} \cdots \frac{\mathbf{1}_{R \times R}}{R} \right) = 1.</script><p>综上所述，$\boldsymbol{T}$ 是一个单位矩阵。</p>
<h4 id="如何正则化"><a href="#如何正则化" class="headerlink" title="如何正则化"></a>如何正则化</h4><p>微调需要正则化，在过去其他模型中，有恒等正则化（identity regularization）和正交正则化（orthogonal regularization）的方法。但是直接计算 $T$ 的正则化是一件比较昂贵的事，通过TRM的性质，我们可以通过高效的计算核心张量 $\mathbf{A}^{1:D}$ 来减少计算开销。</p>
<p>通过Proposition 1，我们可以通过如下方式计算$||\boldsymbol{T} - \boldsymbol{I}||_F$。</p>
<div class="note success no-icon flat"><p><strong>Identity Regularization</strong></p>
<script type="math/tex; mode=display">
\mathcal{R}_I(\mathbf{A}^{1:D}) = \sum_{d=1}^D \sum_{r,r'=1}^R ||\mathbf{A}^d[:,:,r,r'] - \frac{I_{I_d}}{R}||_F</script></div>
<p>对于正交正则化，首先证明一个命题。</p>
<div class="note info no-icon flat"><p><strong>Proposition 2</strong><br>两个 TRM $\boldsymbol{X} = \text{TRM}(\mathbf{A}^{1:D})$ 和 $\boldsymbol{Y} = \text{TRM}(\mathbf{B}^{1:D})$ 的矩阵乘积仍然是一个 TRM，即 $\boldsymbol{X}\boldsymbol{Y}^\mathsf{T} = \text{TRM}(\mathbf{C}^{1:D})$，其中每个核心张量满足：</p>
<script type="math/tex; mode=display">
\mathbf{C}^d[i_d, j_d, :, :] = \sum_{l_d} \mathbf{A}^d[i_d, l_d, :, :] \otimes \mathbf{B}^d[j_d, l_d, :, :]</script><p>对于所有 $d = 1, \dots, D$ 和 $i_d, j_d = 1, \dots, I_d$ 均成立。</p>
</div>
<p><strong>证明：</strong></p>
<p>为简便起见，在此证明中，我们将切片记为 $\mathbf{A}^d[i_d, j_d] = \mathbf{A}^d[i_d, j_d, :, :]$ 以及 $\mathbf{B}^d[i_d, j_d] = \mathbf{B}^d[i_d, j_d, :, :]$。</p>
<p>假设 $\boldsymbol{X}$ 的形状为 $I \times K$，其中 $I = \prod_{d=1}^D I_d$ 且 $K = \prod_{d=1}^D K_d$；$\boldsymbol{Y}$ 的形状为 $J \times K$，其中 $J = \prod_{d=1}^D J_d$。<br>根据 TRM 的定义，我们可以按如下方式计算乘积 $\boldsymbol{Z} = \boldsymbol{X}\boldsymbol{Y}^\mathsf{T}$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{Z}[\overline{i_1 \cdots i_D}, \overline{j_1 \cdots j_D}]
&= \sum_{k_1, \dots, k_D}^{K_1, \dots, K_D} \boldsymbol{X}[\overline{i_1 \cdots i_D}, \overline{k_1 \cdots k_D}] \boldsymbol{Y}[\overline{j_1 \cdots j_D}, \overline{k_1 \cdots k_D}] \\
&= \sum_{k_1, \dots, k_D}^{K_1, \dots, K_D} \text{tr}(\mathbf{A}^1[i_1, k_1] \cdots \mathbf{A}^D[i_D, k_D]) \\
&\quad \quad \quad \quad \cdot \text{tr}(\mathbf{B}^1[j_1, k_1] \cdots \mathbf{B}^D[j_D, k_D]) \\
&= \sum_{k_1, \dots, k_D}^{K_1, \dots, K_D} \text{tr} \left( \{ \mathbf{A}^1[i_1, k_1] \cdots \mathbf{A}^D[i_D, k_D] \} \right. \\
&\quad \quad \quad \quad \left. \otimes \{ \mathbf{B}^1[j_1, k_1] \cdots \mathbf{B}^D[j_D, k_D] \} \right) \\
&= \sum_{k_1, \dots, k_D}^{K_1, \dots, K_D} \text{tr} \left( (\mathbf{A}^1[i_1, k_1] \otimes \mathbf{B}^1[j_1, k_1]) \right. \\
&\quad \quad \quad \quad \left. \cdots (\mathbf{A}^D[i_D, k_D] \otimes \mathbf{B}^D[j_D, k_D]) \right) \\
&= \text{tr} \left\{ \sum_{k_1, \dots, k_D}^{K_1, \dots, K_D} [ (\mathbf{A}^1[i_1, k_1] \otimes \mathbf{B}^1[j_1, k_1]) \right. \\
&\quad \quad \quad \quad \left. \cdots (\mathbf{A}^D[i_D, k_D] \otimes \mathbf{B}^D[j_D, k_D]) ] \right\} \\
&= \text{tr} \left\{ \sum_{k_1}^{K_1} (\mathbf{A}^1[i_1, k_1] \otimes \mathbf{B}^1[j_1, k_1]) \right. \\
&\quad \quad \quad \quad \left. \cdots \sum_{k_D}^{K_D} (\mathbf{A}^D[i_D, k_D] \otimes \mathbf{B}^D[j_D, k_D]) \right\},
\end{aligned}</script><p>上述结果遵循 TR 格式。因此 $\boldsymbol{X}\boldsymbol{Y}^\mathsf{T} = \text{TRM}(\mathbf{C}^{1:D})$，其中每个核心张量 $\mathbf{C}^d[i_d, j_d, :, :] = \sum_{l_d} (\mathbf{A}^d[i_d, l_d, :, :] \otimes \mathbf{B}^d[j_d, l_d, :, :])$。</p>
<p>为了使 $\boldsymbol{X}$ 正交，即 $\boldsymbol{X}\boldsymbol{X}^\mathsf{T} = \boldsymbol{I}$，我们可以根据命题 1 中的初始化方案对 $\mathbf{C}^{1:D}$ 进行正则化。</p>
<p>通过Proposition 2，我们可以通过如下方式计算$||\boldsymbol{TT^{\text{T}}} - \boldsymbol{I}||_F$。</p>
<div class="note success no-icon flat"><p><strong>Orthogonal Regularization</strong></p>
<script type="math/tex; mode=display">
\mathcal{R}_O(\mathbf{A}^{1:D}) = \sum_{d=1}^D \sum_{i_d, j_d=1}^{I_d, J_d} ||\sum_{l=1}^{I_d} (\mathbf{A}^d[i_d, l, :, :] \otimes \mathbf{B}^d[j_d, l, :, :]) - \frac{I_{R^2}}{R}||_F</script></div>
<blockquote>
<p>实践中发现恒等正则化的效果优于正交正则化。</p>
</blockquote>
<h3 id="对残差的TR分解"><a href="#对残差的TR分解" class="headerlink" title="对残差的TR分解"></a>对残差的TR分解</h3><p>我们希望在transform之后，剩下的参差可以用非常紧致的结构近似。这里的 ‘Compact’ (紧致) 并非指拓扑学意义上的紧致性，而是指可以高度压缩参数的技术。</p>
<p>假设残差 $\boldsymbol{\Delta} \in \mathbb{R}^{I \times J}$，这里 $I = \prod_{d=1}^D I_d$，$J = \prod_{d=1}^D J_d$，TR分解将这个矩阵分解为$2D$个三阶核心张量，可记为$\mathbf{B}^{d}\in\mathbb{R}^{I_d\times R\times R}$和$\mathbf{C}^{d}\in\mathbb{R}^{J_d\times R\times R}$</p>
<div class="note info no-icon flat"><p><strong>TR分解：</strong></p>
<script type="math/tex; mode=display">
\boldsymbol{\Delta}[\overline{i_1 \cdots i_D}, \overline{j_1 \cdots j_D}] = \text{tr}(\mathbf{B}^1[i_1, :, :] \cdots \mathbf{B}^D[i_D, :, :] \mathbf{C}^{1}[j_1, :, :] \cdots \mathbf{C}^{D}[j_D, :, :]).</script></div>
<blockquote>
<p>可以计算得到 TR 的空间复杂度是$\mathcal{O}(DI^{1/D}R^2)$。 通过调整合适的超参数$D$和$R$，比TRM的空间复杂度还要更小。</p>
</blockquote>
<h4 id="TR的初始化"><a href="#TR的初始化" class="headerlink" title="TR的初始化"></a>TR的初始化</h4><p>由于高阶结构，TR格式可能对初始化更敏感。先前在PEFT中采用TR的工作对所有因子使用随机高斯初始化，因此失去了像LoRA中那样的整体适配的零初始化。这可能会导致优化不稳定，并导致预训练模型信息的丢失。</p>
<p>特别地，我们可以将TR层表示为一系列线性层。具体做法如下：</p>
<p>给定张量$\mathbf{A}$和$\mathbf{X}$，形状分别为$I_d\times R\times R$和$I_1\times\cdots\times I_d\times R$，我们定义 $\times_2$：</p>
<div class="note success no-icon flat"><p>$\times_2$运算：</p>
<script type="math/tex; mode=display">
\mathbf{A} \times_2 \mathbf{X} = \sum_{l=1}^{I_d}\sum_{r=1}^{R}\mathbf{A}[l,;,r]\mathbf{X}[I_1,\cdots,l,r]</script></div>
<p>这种运算的结果形状为$I_1\times\cdots\times I_{d-1}\times R$，因此我们可以将TR残差层定义为：</p>
<script type="math/tex; mode=display">
\text{TR}(\mathbf{B},\mathbf{C})\mathbf{x} = \text{tr}(\mathbf{B}^1\times_2\cdots\times_2\mathbf{B}^D\times_2\mathbf{C}^1\times_2\cdots\times_2\mathbf{C}^{D}\times_2\mathbf{X})</script><p>为了保证零初始化，我们将$\mathbf{B}^1$初始化为零张量，其余初始化服从高斯分布$\mathcal{N}(0,\sigma^2)$，这里$\sigma$的选取符合$\mu$P 框架，即$\sigma = \Theta(\sqrt{n_{out}}/n_{in})$，这里的$n_{out} = R$，$n_{in} = I_dR$，这样的目的是通过控制权重的谱范数比例，可以确保无论网络多宽，每一层的激活值变化都能保持在一个最佳的量级，既不消失也不爆炸，从而最大化模型的学习效率。</p>
<h2 id="与其他方法的比较"><a href="#与其他方法的比较" class="headerlink" title="与其他方法的比较"></a>与其他方法的比较</h2><p>在先前已有的工作中，主要依赖了一些稀疏或固定的变换，而非这里稠密且可学习的TRM。</p>
<h3 id="DoRA"><a href="#DoRA" class="headerlink" title="DoRA"></a>DoRA</h3><div class="note info no-icon flat"><p><strong>DoRA：</strong></p>
<p>目前非常流行的微调方法，它将权重分解为幅度（Magnitude）和方向（Direction），分别微调它们。</p>
<script type="math/tex; mode=display">
\mathbf{W}' = \dfrac{\mathbf{W}_0 + \mathbf{BA}}{\|\mathbf{W}_0 + \mathbf{BA}\|_c}\cdot \text{diag}(\mathbf{m})</script><p>这里的$\mathbf{m}\in\mathbb{R}^{1\times J}$, $\mathbf{B}\in\mathbb{R}^{I\times R}$和$\mathbf{A}\in\mathbb{R}^{R\times J}$是可训练的参数，$|\cdot|_c$代表列范数。</p>
</div>
<p>作者指出在实际训练的时候，将范数视为常数不进行更新，因此可以将DoRA重写为</p>
<script type="math/tex; mode=display">
\mathbf{W}'\propto \mathbf{W}_0\text{diag}(\mathbf{m}) + \mathbf{BA}\text{diag}(\mathbf{m}) \approx \mathbf{W}_0\text{diag}(\mathbf{m}) + \mathbf{BA}</script><p>这里可以将最后的$\mathbf{m}$融入$\mathbf{A}$中训练</p>
<p>这意味着，DoRA 实际上隐含了一个<strong>对角变换矩阵 </strong> $\text{diag}(m)$。相比之下，TRM是一个更稠密的变换。</p>
<h3 id="采用固定变换的方法"><a href="#采用固定变换的方法" class="headerlink" title="采用固定变换的方法"></a>采用固定变换的方法</h3><p>另一类 PEFT 方法（如 <strong>FouRA</strong>, <strong>BoFT</strong> 等）采用固定的正交变换或频域变换。这类方法假设权重在特定的固定变换下具有更好的低秩结构。例如，FouRA 在变换后的权重空间进行微调：</p>
<div class="note info no-icon flat"><p><strong>FouRA：</strong></p>
<script type="math/tex; mode=display">
\mathbf{y} = \mathcal{F}^{-1}(\mathcal{F}(\mathbf{W}_0) + \mathbf{B} \cdot \mathcal{F}(\mathbf{A}))\mathbf{x}</script><p>其中 $\mathcal{F}$ 是离散傅里叶变换（DFT）矩阵，它是<strong>固定不变的</strong>。</p>
</div>
<p>这类方法应用固定的变换，而TRM采用可学习的变换，能够自适应地学习不同模型和任务之间的这种投影。</p>
<p>下表总结了 TLoRA 在现有方法中的定位：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">方法</th>
<th style="text-align:left">变换类型 (Transform Type)</th>
<th style="text-align:left">变换性质 (Property)</th>
<th style="text-align:left">潜在问题</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>DoRA</strong></td>
<td style="text-align:left">对角矩阵 (Diagonal)</td>
<td style="text-align:left"><strong>可学习</strong> 但 <strong>极度稀疏</strong></td>
<td style="text-align:left">无法建模复杂的特征相关性</td>
</tr>
<tr>
<td style="text-align:left"><strong>FouRA/OFT</strong></td>
<td style="text-align:left">正交/傅里叶矩阵</td>
<td style="text-align:left"><strong>稠密</strong> 但 <strong>固定不可变</strong></td>
<td style="text-align:left">强依赖先验假设，缺乏灵活性</td>
</tr>
<tr>
<td style="text-align:left"><strong>TLoRA</strong></td>
<td style="text-align:left"><strong>Tensor-Ring 矩阵</strong></td>
<td style="text-align:left"><strong>稠密 且 可学习</strong></td>
<td style="text-align:left">兼顾了表达能力与自适应性</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>统计机器学习</category>
      </categories>
      <tags>
        <tag>毕业论文</tag>
        <tag>TR 分解</tag>
        <tag>张量</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>Laplace Equation Note</title>
    <url>/posts/1f9ad889.html</url>
    <content><![CDATA[<p>这篇博客主要是为了测试写博客的功能，最初来自于PDE课程上的课堂笔记。教材是Evans的PDE，封面图来自在Berkeley交换时在Evans Hall借阅的纸质书。耿老师教得很好，考核也很仁慈，现在他已经发了四大了可喜可贺！以下是正文。</p>
<p>Among the most important of all partial differential equations are undoubtedly <em>Laplace’s equation</em></p>
<script type="math/tex; mode=display">
\Delta u = 0</script><p>and <em>Poisson’s equation</em></p>
<script type="math/tex; mode=display">
-\Delta u = f</script><p>First of all we study the properties of <strong>harmonic</strong> functions.</p>
<div class="note info no-icon flat"><p><strong>Definition 1 (Harmonic Function):</strong><br>A $C^2$ function $u$ satisfying $\Delta u = 0$ is called a <em>harmonic function</em>.</p>
</div>
<h2 id="Mean-Value-Formulas"><a href="#Mean-Value-Formulas" class="headerlink" title="Mean-Value Formulas"></a>Mean-Value Formulas</h2><h3 id="Theorem-Mean-value-formulas-for-Laplace’s-equation"><a href="#Theorem-Mean-value-formulas-for-Laplace’s-equation" class="headerlink" title="Theorem: Mean-value formulas for Laplace’s equation"></a>Theorem: Mean-value formulas for Laplace’s equation</h3><div class="note success no-icon flat"><p><strong>Theorem 1 (Mean-value formulas):</strong><br>If $u \in C^2(U)$ is harmonic, then</p>
<script type="math/tex; mode=display">
u(x) = \frac{1}{|\partial B(x,r)|} \int_{\partial B(x,r)} u \, d\sigma = \frac{1}{|B(x,r)|} \int_{B(x,r)} u \, dy</script><p>for each ball $B(x, r) \subset U$.</p>
</div>
<details class="folding-tag" cyan><summary> Proof (Click to expand) </summary>
              <div class='content'>
              <p><strong>Proof:</strong></p><p>Set</p><script type="math/tex; mode=display">\varphi(r) = \frac{1}{|\partial B(x,r)|} \int_{\partial B(x,r)} u(y) \, d\sigma(y)</script><p>where $|\partial B(x,r)| = n\alpha(n) r^{n-1}$, and $\alpha(n) = \frac{\pi^{n/2}}{\Gamma\left(\frac{n}{2} + 1\right)}$.</p><p>By a change of variables $y = x + rz$, we have</p><script type="math/tex; mode=display">\varphi(r) = \frac{r^{n-1}}{n\alpha(n) r^{n-1}} \int_{\partial B(0,1)} u(x + rz) \, d\sigma(z)</script><p>Then,</p><script type="math/tex; mode=display">\varphi'(r) = \frac{1}{n\alpha(n) } \int_{\partial B(0,1)} \nabla u(x + rz) \cdot z \, d\sigma(z)</script><p>By the divergence theorem:</p><script type="math/tex; mode=display">\int_{\partial B(x,r)} \frac{\partial u}{\partial \nu} \, d\sigma = \int_{B(x,r)} \Delta u \, dy</script><p>We have</p><script type="math/tex; mode=display">\begin{aligned}\varphi^{\prime}(r) &= \frac{1}{n\alpha(n) } \int_{\partial B(0,1)} \nabla u(x + rz) \cdot \dfrac{y-x}{r} \, d\sigma(z) \\&= \frac{1}{n\alpha(n)r^{n-1} } \int_{\partial B(x,r)} \nabla u(y) \cdot \nu \, d\sigma(y)\\&= \frac{1}{n\alpha(n)r^{n-1} } \int_{\partial B(x,r)} \dfrac{\partial u (y)}{\partial\nu}  \, d\sigma(y)\\&= \frac{1}{n\alpha(n)r^{n-1} } \int_{ B(x,r)} \Delta u (y)  \, dy\\&= \dfrac{r}{n}\cdot\dfrac{1}{|B(x,r)|}\int_{ B(x,r)} \Delta u (y)  \, dy\end{aligned}</script><p>Since $u$ is harmonic ($\Delta u = 0$), we conclude that $\varphi’(r) = 0$, which implies that $\varphi(r)$ is constant.</p><p>By the Lebesgue Differentiation Theorem</p><script type="math/tex; mode=display">\varphi(r) =\lim_{t \to 0} \frac{1}{|\partial B(x,t)|} \int_{\partial B(x,t)} u \, d\sigma = u(x)</script><p>By employing polar coordinates, we have</p><script type="math/tex; mode=display">\int_{B(x,r)} u \, dy = \int_0^r \int_{\partial B(x,s)} u \, d\sigma \, ds</script><p>and</p><script type="math/tex; mode=display">\int_{B(x,r)} u \, dy = n\alpha(n) \int_0^r s^{n-1} u(x) \, ds</script><p>Finally,</p><script type="math/tex; mode=display">u(x) = \frac{1}{|B(x,r)|} \int_{B(x,r)} u \, dy</script><p><strong>Q.E.D.</strong></p>
              </div>
            </details>
<h3 id="Theorem-Converse-to-Mean-Value-Property"><a href="#Theorem-Converse-to-Mean-Value-Property" class="headerlink" title="Theorem: Converse to Mean Value Property"></a>Theorem: Converse to Mean Value Property</h3><div class="note success no-icon flat"><p><strong>Theorem 2 (Converse to mean-value property):</strong><br>If $u \in C^2(U)$ satisfies</p>
<script type="math/tex; mode=display">
u(x) = \frac{1}{|\partial B(x,r)|} \int_{\partial B(x,r)} u \, dS</script><p>for each ball $B(x,r) \subset U$, then $u$ is harmonic.</p>
</div>
<details class="folding-tag" cyan><summary> Proof (Click to expand) </summary>
              <div class='content'>
              <p><strong>Proof:</strong><br>Suppose not. WLOG, there exists a ball $B(x, r) \subset U$, such that $\Delta u &gt; 0$ in $B(x, r)$.</p><p>Define</p><script type="math/tex; mode=display">\varphi(r) = \dfrac{1}{|B(x,r)|}\int_{\partial B(x,r)} u \, d\sigma</script><p>Then</p><script type="math/tex; mode=display">\varphi'(r) = \dfrac{r}{n}\cdot\dfrac{1}{|B(x,r)|}\int_{ B(x,r)} \Delta u (y)  \, dy > 0</script><p>However, since $\varphi(r)$ is a constant, $\varphi’(r) = 0$, which is a contradiction (as $\Delta u &gt; 0$).<br>Therefore, $u$ must be harmonic. <strong>Q.E.D.</strong></p>
              </div>
            </details>
<h2 id="Maximum-Principle"><a href="#Maximum-Principle" class="headerlink" title="Maximum Principle"></a>Maximum Principle</h2><div class="note success no-icon flat"><p><strong>Theorem 3 (Strong maximum principle):</strong><br>Suppose $u \in C^2(U) \cap C(\overline{U})$ is harmonic within $U$.</p>
<ol>
<li><strong>Then</strong><script type="math/tex; mode=display">
\max_{\overline{U}} u = \max_{\partial U} u</script></li>
<li><strong>Furthermore</strong>, if $U$ is connected and there exists a point $x_0 \in U$ such that<script type="math/tex; mode=display">
u(x_0) = \max_{\overline{U}} u</script>then $u$ is constant within $U$.</li>
</ol>
<p>Assertion (i) is the maximum principle for Laplace’s equation and (ii) is the strong maximum principle. Replacing $u$ by $-u$, we recover also similar assertions with “min” replacing “max”.</p>
</div>
<details class="folding-tag" cyan><summary> Proof (Click to expand) </summary>
              <div class='content'>
              <p><strong>Proof:</strong><br>Suppose $\exists x<em>0 \in U$ such that $u(x_0) = \max</em>{\overline{U}} u := M$.</p><p>Then for $0 &lt; r &lt; \delta(x_0) = \operatorname{dist}(x_0, \partial  U)$, the mean value property implies that</p><script type="math/tex; mode=display">u(x_0) = \frac{1}{|B(x_0, r)|} \int_{B(x_0, r)} u(x) \, dx \leq M</script><p>Since $u(x_0) = M$, equality holds iff $u(x) = M$ for all $x \in B(x_0, r)$.</p><p>Repeating this argument, we see that $u(y) = M$ for all $y \in B(x_0, r)$.</p><p>Hence the set ${ x \in U \mid u(x) = M }$ is both open and relatively closed in $U$, and thus equals $U$ if $U$ is connected. <strong>Q.E.D.</strong></p>
              </div>
            </details>
<p>Before we learn the weak maximum principle, we first introduce the following definition:</p>
<div class="note info no-icon flat"><p><strong>Definition 2 (Subharmonic and Superharmonic Functions):</strong><br>Let $u$ be a $C^2$ function in $U$. Then $u$ is a <strong>subharmonic</strong> (superharmonic) function in $U$ if $\Delta u \geq 0$ ($\Delta u \leq 0$).</p>
</div>
<p>Subharmonic and superharmonic functions both have maximum principle, we only show one of it.</p>
<div class="note success no-icon flat"><p><strong>Theorem 4 (Maximum Principle for Subharmonic Functions):</strong><br>Let $U$ be a bounded domain in $\mathbb{R}^n$ and $u \in C^2(U) \cap C(\overline{U})$ be subharmonic in $U$. Then $u$ attains its maximum in $\overline{U}$, i.e.,</p>
<script type="math/tex; mode=display">
\max_{\overline{U}} u = \max_{\partial U} u</script></div>
<details class="folding-tag" cyan><summary> Proof (Click to expand) </summary>
              <div class='content'>
              <p><strong>Proof:</strong></p><ol><li><p>First, consider $\Delta u &gt; 0$ in $U$. If $u$ has a local maximum at a point $x_0 \in U$, then the Hessian matrix $\nabla^2 u(x_0)$ is negative semi-definite. Thus,</p><script type="math/tex; mode=display">\Delta u(x_0) = \operatorname{tr}(\nabla^2 u(x_0)) \leq 0</script><p>which is a contradiction.</p></li><li><p>Now consider $\Delta u \geq 0$. To handle this, for any $\varepsilon &gt; 0$, define</p><script type="math/tex; mode=display">u_\varepsilon(x) = u(x) + \varepsilon |x|^2</script><p>Then,</p><script type="math/tex; mode=display">\Delta u_\varepsilon(x) = \Delta u(x) + \Delta (\varepsilon |x|^2) = \Delta u(x) + 2n\varepsilon > 0</script><p>By Step 1, we have</p><script type="math/tex; mode=display">\max_{\overline{U}} u_\varepsilon = \max_{\partial U} u_\varepsilon</script></li></ol><p>Observe that</p><script type="math/tex; mode=display">\max_{\overline{U}} u \leq \max_{\overline{U}} u_\varepsilon = \max_{\partial U} u_\varepsilon \leq \max_{\partial U} u + \max_{\partial U} (\varepsilon |x|^2)</script><p>Taking $\varepsilon \to 0$, we conclude</p><script type="math/tex; mode=display">\max_{\overline{U}} u = \max_{\partial U} u</script><p><strong>Q.E.D.</strong></p>
              </div>
            </details>
<div class="note success no-icon flat"><p><strong>Theorem 5 (Uniqueness):</strong><br>Let $g \in C(\partial U)$, $f \in C(U)$. Then there exists at most one solution $u \in C^2(U) \cap C(\overline{U})$ of the boundary-value problem</p>
<script type="math/tex; mode=display">
\begin{cases}
    -\Delta u = f & \text{in } U, \\
    u = g & \text{on } \partial U.
\end{cases}</script></div>
<details class="folding-tag" cyan><summary> Proof (Click to expand) </summary>
              <div class='content'>
              <p><strong>Proof:</strong><br>If $u$ and $\tilde{u}$ are both solutions for the boundary-value problem , apply maximum principle to the harmonic functions $w := \pm (u - \tilde{u})$.</p>
              </div>
            </details>
<div class="note warning no-icon flat"><p><strong>Remark:</strong> If $U$ is unbounded, the conclusion fails.</p>
</div>
<h2 id="Regularity"><a href="#Regularity" class="headerlink" title="Regularity"></a>Regularity</h2><p>Our main result in this section is that if $u\in C^{2}$ is harmonic, then necessarily $u\in C^{\infty}$.</p>
<p>Let $\Omega \subset \mathbb{R}^n$ be open. For $\varepsilon &gt; 0$, denote</p>
<script type="math/tex; mode=display">
\Omega_\varepsilon = \{ x \in \Omega \mid \text{dist}(x, \partial \Omega) > \varepsilon \}</script><div class="note info no-icon flat"><p><strong>Definition 3 (Mollifier):</strong><br>Define the mollifier as:</p>
<script type="math/tex; mode=display">
\eta(x) =
\begin{cases}
    C \exp\left(\frac{1}{ |x|^2 - 1}\right), & \text{if } |x| < 1, \\
    0, & \text{if } |x| \geq 1.
\end{cases}</script><p>where $\eta \in C^\infty(\mathbb{R}^n)$ and satisfies $\int_{\mathbb{R}^n} \eta(x) \, dx = 1$.</p>
<p>Let</p>
<script type="math/tex; mode=display">
\eta_\varepsilon(x) = \frac{1}{\varepsilon^n} \eta\left(\frac{x}{\varepsilon}\right)</script><p>This is called the <em>standard mollifier</em>.</p>
</div>
<p>By definition, we know that $\eta<em>{\varepsilon}$ has support, and $spt(\eta</em>{\varepsilon})\subset B(0,\varepsilon)$.</p>
<div class="note info no-icon flat"><p><strong>Definition 4 (Mollification):</strong><br>For a function $f$, define its mollification as:</p>
<script type="math/tex; mode=display">
f^\varepsilon = \eta_\varepsilon * f = \int_{\Omega} \eta_\varepsilon(x-y) f(y) \, dy = \int_{B(0,\varepsilon)} \eta_\varepsilon(y) f(x-y) \, dy</script></div>
<div class="note success no-icon flat"><p><strong>Theorem 6 (Properties of Mollifiers):</strong> $f^{\varepsilon}\in C^{\infty}(\Omega_{\varepsilon})$.</p>
</div>
<details class="folding-tag" cyan><summary> Proof (Click to expand) </summary>
              <div class='content'>
              <p><strong>Proof:</strong><br>We can rewrite:</p><script type="math/tex; mode=display">f^\varepsilon(x) = \int_{\Omega} \eta_\varepsilon(y) f(x - y) \, dy = \frac{1}{\varepsilon^n} \int_{\Omega} \eta\left(\frac{y}{\varepsilon}\right) f(x - y) \, dy</script><p>Change variables with $z = \frac{y}{\varepsilon}$, $dy = \varepsilon^n dz$:</p><script type="math/tex; mode=display">f^\varepsilon(x) = \int_{\mathbb{R}^n} \eta(z) f(x - \varepsilon z) \, dz</script><p>Since $\eta \in C^\infty$, we conclude $f^\varepsilon \in C^\infty$.</p><p>In fact, fix $x \in U<em>\varepsilon$, $i \in { 1, \dots, n }$, and $h$ so small that $x + h e_i \in U</em>\varepsilon$. Then:</p><script type="math/tex; mode=display">\frac{f^\varepsilon(x + h e_i) - f^\varepsilon(x)}{h} = \frac{1}{\varepsilon^n} \int_\Omega \frac{1}{h} \left[ \eta\left(\frac{x + h e_i - y}{\varepsilon}\right) - \eta\left(\frac{x - y}{\varepsilon}\right)\right] f(y) \, dy</script><script type="math/tex; mode=display">= \frac{1}{\varepsilon^n} \int_V \frac{1}{h} \left[ \eta\left(\frac{x + h e_i - y}{\varepsilon}\right) - \eta\left(\frac{x - y}{\varepsilon}\right)\right] f(y) \, dy</script><p>for some open set $V \subset\subset U$. As</p><script type="math/tex; mode=display">\frac{1}{h} \left[ \eta\left(\frac{x + h e_i - y}{\varepsilon}\right) - \eta\left(\frac{x - y}{\varepsilon}\right)\right] \to \frac{1}{\varepsilon} \eta_{x_i}\left(\frac{x - y}{\varepsilon}\right)</script><p>uniformly on $V$, the partial derivative $f^\varepsilon_{x_i}(x)$ exists and equals</p><script type="math/tex; mode=display">f^\varepsilon_{x_i}(x) = \int_\Omega \eta_{\varepsilon, x_i}(x - y) f(y) \, dy</script><p>A similar argument shows that $D^\alpha f^\varepsilon(x)$ exists, and</p><script type="math/tex; mode=display">D^\alpha f^\varepsilon(x) = \int_\Omega D^\alpha \eta_\varepsilon(x - y) f(y) \, dy, \quad (x \in U_\varepsilon)</script><p>for each multiindex $\alpha$. <strong>Q.E.D.</strong></p>
              </div>
            </details>
<div class="note success no-icon flat"><p><strong>Theorem 7 (Smoothness):</strong><br>Suppose $u \in C^2(\Omega)$ and $\Delta u = 0$ in $\Omega$. Then $u \in C^\infty(\Omega)$.</p>
</div>
<details class="folding-tag" cyan><summary> Proof (Click to expand) </summary>
              <div class='content'>
              <p><strong>Proof:</strong><br>Let $\eta$ be the standard mollifier. Define</p><script type="math/tex; mode=display">u^\varepsilon(x) = \eta_\varepsilon * u = \int_{\Omega} \eta_\varepsilon(x-y) u(y) \, dy = \frac{1}{\varepsilon^n} \int_{B(x, \varepsilon)} \eta\left(\frac{x-y}{\varepsilon}\right) u(y) \, dy</script><p>Using polar coordinates and the mean value property:</p><script type="math/tex; mode=display">\begin{aligned}    u^\varepsilon(x) &= \frac{1}{\varepsilon^n} \int_{0}^{\varepsilon}\int_{\partial B(x, r)} \eta\left(\frac{r}{\varepsilon}\right) u(y) \, d\sigma dr \\    &= \frac{1}{\varepsilon^n} \int_{0}^{\varepsilon} \eta\left(\frac{r}{\varepsilon}\right) \frac{n\alpha(n) r^{n-1}}{n\alpha(n) r^{n-1}} \int_{\partial B(x, r)} u(y) \, d\sigma dr\\    &=\frac{1}{\varepsilon^n} \int_{0}^{\varepsilon} \eta\left(\frac{r}{\varepsilon}\right) n\alpha(n) r^{n-1}u(x) \, dr\\    &= u(x)\frac{1}{\varepsilon^n} \int_{0}^{\varepsilon}\int_{\partial B(x,r)} \eta\left(\frac{r}{\varepsilon}\right) \, d\sigma dr\\    &= u(x)\int_{B(x,\varepsilon)}\dfrac{1}{\varepsilon^{n}}\eta\left(\frac{r}{\varepsilon}\right) \, dr\\    &= u(x)\in C^{\infty}(\Omega_{\varepsilon})\end{aligned}</script><p><strong>Q.E.D.</strong></p>
              </div>
            </details>
<p>The stronger conclusion is that:</p>
<div class="note success no-icon flat"><p><strong>Theorem 8 (Analyticity):</strong><br>Assume $u$ is harmonic in $\Omega$, then $u$ is analytic in $\Omega$.</p>
</div>
<h2 id="Interior-Estimate"><a href="#Interior-Estimate" class="headerlink" title="Interior Estimate"></a>Interior Estimate</h2><div class="note success no-icon flat"><p><strong>Theorem 9 (Estimates on Derivatives):</strong><br>Assume $u$ is harmonic in $U$. Then:</p>
<script type="math/tex; mode=display">
|D^\alpha u(x_0)| \leq \frac{C_k}{r^{n+k}} \| u \|_{L^1(B(x_0, r))}</script><p>for each ball $B(x_0, r) \subset U$ and each multiindex $\alpha$ of order $|\alpha| = k$. </p>
<p>Here:</p>
<script type="math/tex; mode=display">
C_0 = \frac{1}{\alpha(n)}, \quad C_k = \frac{(2^{n+1}nk)^k}{\alpha(n)}, \quad (k = 1, 2, \dots)</script><p>where $\alpha(n)$ is the volume of the unit ball in $\mathbb{R}^n$.</p>
</div>
<details class="folding-tag" cyan><summary> Proof (Click to expand) </summary>
              <div class='content'>
              <p><strong>Proof:</strong><br>We argue by induction on $k$.</p><ol><li><p>For $k = 0$:</p><script type="math/tex; mode=display">\text{LHS} = u(x_0), \quad \text{RHS} = \frac{1}{\alpha(n) r^n} \int_{B(x_0, r)} u</script><p>Since $u$ is harmonic, by the mean value property:</p><script type="math/tex; mode=display">|u(x_0)| = \frac{1}{\alpha(n) r^n} \int_{B(x_0, r)} |u| \leq \frac{1}{\alpha(n) r^n} \int_{B(x_0, r)} |u|</script></li><li><p>For $k = 1$:</p><script type="math/tex; mode=display">C_1 = \frac{2^n n}{\alpha(n)}</script><p>Observe that $\frac{\partial u}{\partial x_i}$ is still harmonic, since $\Delta \frac{\partial u}{\partial x_i} = \frac{\partial}{\partial x_i} (\Delta u) = 0$.</p></li></ol><p>By the mean value property:</p><script type="math/tex; mode=display">\frac{\partial u}{\partial x_i}(x_0) = \frac{1}{\alpha(n) r^n} \int_{B(x_0, r)} \frac{\partial u}{\partial x_i}</script><p>Using the divergence theorem:</p><script type="math/tex; mode=display">\frac{\partial u}{\partial x_i}(x_0) = \frac{1}{\alpha(n) r^n} \int_{\partial B(x_0, r)} u \, \nu_i</script><p>where $\nu_i$ is the $i$-th component of the outward unit normal. Then:</p><script type="math/tex; mode=display">\left| \frac{\partial u}{\partial x_i}(x_0) \right| \leq \frac{2^n}{\alpha(n) r^{n+1}} \| u \|_{L^1(B(x_0, r))}</script><p>Combining these inequalities, we deduce:</p><script type="math/tex; mode=display">| \nabla u(x_0) | \leq \frac{2^n n}{\alpha(n) r^{n+1}} \| u \|_{L^1(B(x_0, r))}</script><ol><li>Inductive Step:<br>Assume the estimate holds for $k-1$. Then for $|\alpha| = k$, apply similar arguments using the derivatives of $u$, the divergence theorem, and scaling properties to obtain:<script type="math/tex; mode=display">|D^\alpha u(x_0)| \leq \frac{C_k}{r^{n+k}} \| u \|_{L^1(B(x_0, r))}</script></li></ol><p>Thus, the result follows by induction. <strong>Q.E.D.</strong></p>
              </div>
            </details>]]></content>
      <categories>
        <category>PDE</category>
      </categories>
      <tags>
        <tag>Harmonic Analysis</tag>
        <tag>Laplace Equation</tag>
      </tags>
  </entry>
</search>
